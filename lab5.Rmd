---

title: "Journal 1"
#bibliography: references.bib
author: "Dávid Sümeghy"
output: 
  html_document:
    css: tweaks.css
    toc:  true
    toc_float: true
    number_sections: false
  
 
---
Assignment

For this BIGSSS we will use GIS data from Statistics Netherlands, at the municipality, district (‘wijk’), (‘buurt’) and 100-by-100 meter grid level. What are the basic requirements of data necessary and sufficient to determine the level of spatial segregation in specific areas of the Netherlands? Make a list of requirements for non-spatial and spatial measures.

Non-overlapping boundaries,data about groups with common characteristic (ethnicity, religion,income..), spatial location of group members, population density,measurable distances between group members, data to analyze the relative location of a group in a city (for centrality and polycentrality measurements).

How would you theoretically want to define the spatial distance between voters/citizens in the Netherlands?
I would like to define the spatial distances with a distance matrix that takes into account  geographical distances and assigns more weight to nearby units (distance decay function). As the crow flies distances, travel distances would be better it takes into account boundaries

Please formulate a precise definition of what you consider to be the relevant neighbourhood (or phrased otherwise ‘local environment’ or ‘social environment’) as input for the segregation measures.
The physical space that an individual uses on a daily basis, because this is the space where there is a chance of exposure or contact with another group. For this reason I would suggest to analyze daily commuting distances, but this can only be known through individual questioning. The problem in defining the local environment may be the existence of nearly impassable boundaries (rivers, busy roads) that separate areas close to each other.

Which theoretical article on residential segregation should we definitely all read for this summer school? Please provide a reference and motivate your answer.

I would recommend the study by Cory McCartan and colleagues (2021), which does not deal with the measurement of segregation itself, but mainly with the definition of the local environment. Based on a questionnaire survey, the authors find that individuals are more likely to identify areas inhabited by people belonging to the same ethnic group and supporting the same party as their own neighbourhood. This means that the local environment is not delimited by a single distance radius, but may extend into spaces that contain similar individuals, but a bit further away. This concept of local environment for individuals already provides scope for segregation and political polarization.

McCartan, C., Brown, J. R., & Imai, K. (2021). Measuring and Modeling Neighborhoods. arXiv preprint arXiv:2110.14014

See the dataframe popcounts and the weight matrix weights below. popcounts contains the population density for two groups at 10 locations. The weights matrix contains the proximity of these points, see (3.1). Based on these ingredients, construct the local environment of each location. That is, the spatial proportions (3.3)) with respect to these two groups at each location.

```{r}
set.seed(567732)
g1 <- sample(20:400, 10)  #counts group 1
g2 <- sample(20:400, 10)  #counts group 2
popcounts <- data.frame(g1, g2)
distances <- matrix(sample(20:400, 100), nrow = 10, ncol = 10)
distances[lower.tri(distances)] <- (t(distances)[lower.tri(distances)])
weights <- exp(-distances/100)
diag(weights) <- 0
rm(list = c("g1", "g2", "distances"))
weights
```

local<-weights%*%as.matrix(popcounts) # matrix algebra


Exposure/Isolation index P∗

$$P*=\frac{\tau^{m}*\frac{\sum_{i}\tau_{i}^m*\tilde{\pi}_{i}^n}{\sum_{i}*\tau_{i}^m}+\tau^{n}*\frac{\sum_{i}\tau_{i}^n*\tilde{\pi}_{i}^m}{\sum_{i}\tau_{i}^n}}{\tau^{m}+\tau^{n}}$$
Clean-up

```{r}
rm(list = ls())
```

General custom functions

```{r}
fsave <- function(x, file, location = "./data/processed/", ...) {
    if (!dir.exists(location))
        dir.create(location)
    datename <- substr(gsub("[:-]", "", Sys.time()), 1, 8)
    totalname <- paste(location, datename, file, sep = "")
    print(paste("SAVED: ", totalname, sep = ""))
    save(x, file = totalname)
}

fpackage.check <- function(packages) {
    lapply(packages, FUN = function(x) {
        if (!require(x, character.only = TRUE)) {
            install.packages(x, dependencies = TRUE)
            library(x, character.only = TRUE)
        }
    })
}

colorize <- function(x, color) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
}
```
Load necessary packages

```{r}
packages = c("tidyverse", "rgl", "spdep", "geosphere", "truncnorm", "progress", "foreach", "doParallel",
    "ape", "seg", "rgl", "OasisR", "compiler")

fpackage.check(packages)
```

Moran’s I function
```{r}
# let us define a Moran's I function (heavily based on Moran.I of package ape) you can toggle
# rowstandardization
fMoranI <- function(x, y = NULL, weight, scaled = FALSE, na.rm = FALSE, alternative = "two.sided", rowstandardize = TRUE) {
    if (is.null(y)) {
        y <- x
    }

    if (dim(weight)[1] != dim(weight)[2])
        stop("'weight' must be a square matrix")
    nx <- length(x)
    ny <- length(y)
    if (dim(weight)[1] != nx | dim(weight)[1] != ny)
        stop("'weight' must have as many rows as observations in 'x' (and 'y', for the bivariate case) ")
    ei <- -1/(nx - 1)
    nas <- is.na(x) | is.na(y)
    if (any(nas)) {
        if (na.rm) {
            x <- x[!nas]
            y <- y[!nas]
            nx <- length(x)
            weight <- weight[!nas, !nas]
        } else {
            warning("'x' and/or 'y' have missing values: maybe you wanted to set na.rm = TRUE?")
            return(list(observed = NA, expected = ei, sd = NA, p.value = NA))
        }
    }
    if (rowstandardize) {
        ROWSUM <- rowSums(weight)
        ROWSUM[ROWSUM == 0] <- 1
        weight <- weight/ROWSUM
    }
    s <- sum(weight)
    mx <- mean(x)
    sx <- x - mx
    my <- mean(y)
    sy <- y - my
    v <- sum(sx^2)
    cv <- sum(weight * sx %o% sy)
    obs <- (nx/s) * (cv/v)
    cv_loc <- rowSums(weight * sx %o% sy)
    obs_loc <- (nx/s) * (cv_loc/v)
    if (scaled) {
        i.max <- (nx/s) * (sd(rowSums(weight) * sx)/sqrt(v/(nx - 1)))
        obs <- obs/i.max
        obs_loc <- obs_loc/i.max
    }
    S1 <- 0.5 * sum((weight + t(weight))^2)
    S2 <- sum((apply(weight, 1, sum) + apply(weight, 2, sum))^2)
    s.sq <- s^2
    k <- (sum(sx^4)/nx)/(v/nx)^2
    sdi <- sqrt((nx * ((nx^2 - 3 * nx + 3) * S1 - nx * S2 + 3 * s.sq) - k * (nx * (nx - 1) * S1 - 2 *
        nx * S2 + 6 * s.sq))/((nx - 1) * (nx - 2) * (nx - 3) * s.sq) - 1/((nx - 1)^2))
    alternative <- match.arg(alternative, c("two.sided", "less", "greater"))
    pv <- pnorm(obs, mean = ei, sd = sdi)
    if (alternative == "two.sided")
        pv <- if (obs <= ei)
            2 * pv else 2 * (1 - pv)
    if (alternative == "greater")
        pv <- 1 - pv
    list(observed = obs, expected = ei, sd = sdi, p.value = pv, observed_locals = obs_loc)


}
fMoranI <- cmpfun(fMoranI)
```

Density corrected Moran’s I function

```{r}
# Density corrected Moran's I.
fMoranIdens <- function(x, y = NULL, proxmat, dens = NULL, N = length(x)) {
    # Adapted from Anselin (1995, eq. 7, 10, 11)
    # https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1538-4632.1995.tb00338.x dens: the
    # proportion of individuals in each cell over the district population if individual level data
    # dens is.null and N is simply length of input if we have aggregate data then N should be total
    # population size (or actually just a large number)
    if (is.null(y)) {
        y <- x
    }
    if (is.null(dens)) {
        dens <- rep(1/N, times = N)
    }

    # correct scaling of opinions for densities #this is really inefficient, should use weighted
    # var from hmsci
    v1dens_ind <- rep(x, times = (dens * N))
    v1dens <- (x - mean(v1dens_ind))/sd(v1dens_ind)
    v2dens_ind <- rep(y, times = (dens * N))
    v2dens <- (y - mean(v2dens_ind))/sd(v2dens_ind)

    # (density) weighted proximity matrix
    w <- proxmat
    wdens <- t(dens * t(w))
    wdens <- wdens/rowSums(wdens)

    # density and proximity weighted locals
    localI <- (v1dens * wdens %*% v2dens)  #formula 7

    # correct the normalization constants
    m2 <- sum(v1dens^2 * dens)
    S0 <- N  #we know the weight matrix for the individual level should add up to N
    ydens <- S0 * m2
    globalI <- sum(localI * dens * N)/ydens  # formula 10/11

    return(list(globalI = globalI, localI = as.numeric(localI)))
}
fMoranIdens <- cmpfun(fMoranIdens)
```

Creating virtual segregated worlds to test segregation measurements

```{r}
# version 09-06-2007


# function define world
iniworld <- function(N = 2000, cn = 4, h = 1, tc = 0.9, pg = c(0.5, 0.5), distropTN = TRUE, plotworld = TRUE,
    seed = NULL) {
    # N= number of agents (even number) cn= number of clusters (even number) h= cluster homogeneity
    # (0.5-1) tc= thinning constant. .9 means retain 90% pg= proportion of groups; length is number
    # of groups distropTN= use truncated normal to generate opinions, default = false

    # in paper opinions [0,1], here [-1,1] in paper tc is 1 - tc

    if (is.null(seed))
        seed <- sample(45667:159876, 1)

    set.seed(seed)

    N_ori <- N

    # functions
    spher_to_cart <- function(r, theta, phi) {
        x = r * cos(phi) * sin(theta)
        y = r * sin(theta) * sin(phi)
        z = r * cos(theta)
        coordinatesxyz <- matrix(c(x, y, z), ncol = 3)
        return(coordinatesxyz)
    }

    distl <- function(x) {
        distVincentySphere(x, matlonglat, r = 1)
    }

    # if tc<1 we need to increase initial N, make sure to keep even number
    if (tc < 1) {
        N <- trunc(N/(tc * 10)) * 10
    }

    # define (random) position of agents on sphere:
    # http://mathworld.wolfram.com/SpherePointPicking.html
    r <- 1
    phi <- 2 * pi * runif(N)
    theta <- acos(2 * runif(N) - 1)
    coordinatesxyz <- spher_to_cart(r, theta, phi)

    phi_r <- (360 * phi)/(2 * pi)
    theta_r <- (180 * theta)/pi
    lat <- 90 - theta_r
    long <- ifelse(phi_r >= 0 & phi_r < 180, -phi_r, abs(phi_r - 360))

    matlonglat <- matrix(c(long, lat), ncol = 2)

    # improve: we only need to calculate half
    matlonglatlist <- lapply(seq_len(nrow(matlonglat)), function(i) matlonglat[i, ])

    distl <- function(x) {
        distVincentySphere(x, matlonglat, r = 1)
    }

    matdist <- sapply(matlonglatlist, distl)

    # model segregation: could be improved. check existing packages.
    parents <- sample(1:N, cn)
    groups <- rep(NA, N)
    # fix if cn==1
    groups[parents] <- sample(c(rep(1, round(cn * pg[1])), rep(-1, cn - round(cn * pg[1]))), cn, replace = FALSE)

    # to whom do children belong
    clusterchildren <- rep(NA, N)

    for (i in c(1:N)) {
        if (!(i %in% parents)) {
            # which parents is closest
            clusterchildren[i] <- parents[which(matdist[i, parents] == min(matdist[i, parents]))]
            # give child same initial value as closest parent
            group <- groups[clusterchildren[i]]
            # change value child depending of cluster homogeneity
            groups[i] <- ifelse(group == -1, sample(c(-1, 1), 1, prob = c(h, 1 - h)), sample(c(-1, 1),
                1, prob = c(1 - h, h)))
        }
    }

    # define opinions of agents
    if (distropTN == TRUE) {
        opinions <- rtruncnorm(N, a = -1, b = 1, mean = 0, sd = 0.45)
    }
    # if(distropTN==FALSE) {opinions <- runif(N, min = -1, max = 1)}

    # for (future) plotting
    color <- ifelse(groups == 1, "blue", "red")

    # thin clusters, make cluster boundaries sharper
    if (tc < 1) {
        childIDi <- sampletc <- NA
        # put in big function
        for (i in 1:cn) {
            childIDi <- which(clusterchildren == parents[i])
            distchildparenti <- matdist[parents[i], childIDi]
            # samplei <- sample(childIDi, trunc(tc*length(childIDi)),
            # prob=exp(-distchildparenti)^2)
            cutoffdistance <- quantile(distchildparenti, tc)
            samplei <- childIDi[distchildparenti < cutoffdistance]
            sampletc <- c(sampletc, samplei)
        }
        clusterchildren <- sampletc <- sampletc[-1]
        sampletc <- c(sampletc, parents)
        N_obs <- length(sampletc)
    }

    N <- N_ori  #setting back to original input

    if (tc == 1) {
        sampletc <- NA
        N_obs <- N_ori
    }

    if (plotworld & tc == 1) {
        .check3d()
        rgl.close()
        plot3d(coordinatesxyz, col = color, box = FALSE, axes = FALSE, xlab = "", ylab = "", zlab = "",
            size = 8, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
        rgl.spheres(0, 0, 0, radius = 0.995, color = "grey")
    }

    if (tc == 1) {
        worldlist <- list(seed, coordinatesxyz, color, groups, opinions, matdist, N, cn, h, tc, pg, N_obs,
            parents, clusterchildren, matlonglat)
        names(worldlist) <- c("seed", "coordinatesxyz", "color", "groups", "opinions", "matdist", "N",
            "cn", "h", "tc", "pg", "N_obs", "parents", "clusterchildren", "matlonglat")
        return(worldlist)
    }

    if (plotworld & tc < 1) {
        .check3d()
        rgl.close()
        plot3d(coordinatesxyz[sampletc, ], col = color[sampletc], box = FALSE, axes = FALSE, xlab = "",
            ylab = "", zlab = "", size = 8, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
        rgl.spheres(0, 0, 0, radius = 0.995, color = "grey")
    }

    if (tc < 1) {
        worldlist <- list(seed, coordinatesxyz[sampletc, ], color[sampletc], groups[sampletc], opinions[sampletc],
            matdist[sampletc, sampletc], N, cn, h, tc, pg, N_obs, parents, clusterchildren, matlonglat[sampletc,
                ])
        names(worldlist) <- c("seed", "coordinatesxyz", "color", "groups", "opinions", "matdist", "N",
            "cn", "h", "tc", "pg", "N_obs", "parents", "clusterchildren", "matlonglat")
        return(worldlist)
    }


}
```


Please calculate White’s spatial proximity by hand (that is, via your own R code)


```{r,eval=FALSE}
pmm <-size <- rep(NA,length(unique(df[ , column]))) # in this object i am saving the mean distance
```

```{r,eval=FALSE}
fSP<-function(df,column,distances){   #creating SP function
for (i in 1:length(unique(df[ ,column]))){  # loop for each unique group
 subs <- df[ , column]==unique(df[ ,column])[i] # define subtracts as the unique groups
pmm[i]<-mean(distances[subs,subs]) # calculate mean distance between the members of the same group
size[i]<-sum(subs) # calculate group sizes
  }
pi_m_t_p_mm<-sum(pmm*size)/sum(size)   #calculate pi_m * p_mm
SP<-pi_m_t_p_mm/mean(distances)
}
```

Demonstrate measures on one world

```{r}
# define parameters
N <- c(100, 200, 400)
cn <- c(4, 8, 16)
h <- c(0.6, 0.7, 0.8)
tc <- c(0.6, 0.7, 0.8)
pg <- c(0.5, 0.6, 0.7)

# run the loop in parallel
n.cores <- parallel::detectCores() - 1  #save one core for other work
# create the cluster
my.cluster <- parallel::makeCluster(n.cores, type = "PSOCK")
# register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)


# to get the same results
set.seed(5893743)

# make sure to define the correct folder beforehand
dataworlds <- foreach(Nsim = N, i = icount()) %:% foreach(cnsim = cn, j = icount()) %:% foreach(hsim = h,
    k = icount()) %:% foreach(tcsim = tc, l = icount()) %:% foreach(pgsim = pg, m = icount(), .packages = packages,
    .inorder = TRUE) %dopar% {
    world <- iniworld(N = Nsim, cn = cnsim, h = hsim, tc = tcsim, pg = pgsim, plotworld = FALSE, seed = NULL)
    save(world, file = paste("./data/processed/worlds/worldN", Nsim, "cn", cnsim, "h", hsim, "tc", tcsim,
        "pg", pgsim, "rda", sep = ""), compress = "bzip2")
    # return(test)
}
```

Plotting the world in 3D

```{r,eval=FALSE}
load(paste("./data/processed/worlds/worldN", N[3], "cn", cn[3], "h", h[3], "tc", tc[3], "pg", pg[2],
    "rda", sep = ""))
str(world)
```
Plotting the world as a projectd map

```{r,eval=FALSE}
plot <- {
    plot3d(world$coordinatesxyz, col = world$color, box = FALSE, axes = FALSE, xlab = "", ylab = "",
        zlab = "", size = 4, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
    rgl.spheres(0, 0, 0, radius = 0.99, color = "grey")
}
```

retrieve data from simulated world

```{r,eval=FALSE}
test <- world
# first define data.
mydf <- as.data.frame(cbind(as.numeric(test$groups == 1), as.numeric(test$groups == -1)))
# define the coordinates. (note: this are from a sphere)
mycoordinates <- test$matlonglat
mydf$Longitude <- test$matlonglat[, 1]
mydf$Latitude <- test$matlonglat[, 2]
points = st_as_sf(mydf, coords = c("Longitude", "Latitude"), crs = 4326)
graticule = st_graticule(lat = seq(-80, 80, 10), lon = seq(-180, 180, 10))
robinson = "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
projected = st_transform(points, robinson)
graticule = st_transform(graticule, robinson)
{
    plot(projected$geometry, pch = 16, col = test$color, reset = FALSE)
    plot(graticule$geometry, col = "#00000040", add = T)
}
```
Setting the slope of the exponential distance decay function

```{r,eval=FALSE}
geodistances <- world$matdist
# we set distance to yourself to 0, be aware that in the weight matrix we may or may not want to
# set weight to yourself as 1.
diag(geodistances) <- 0
# retrieve the group membership
mydf <- as.data.frame(cbind(as.numeric(world$groups == 1), as.numeric(world$groups == -1)))
# define the coordinates
mycoordinates <- world$matlonglat
```


```{r,eval=FALSE}
s <- 1
```

Segregation measures via package seg

```{r,eval=FALSE}
geodistances <- world$matdist
diag(geodistances) <- 0
mygeodistances <- as.dist(geodistances)  #the class of the distance matrix should be dist.              

# explain!
myenv <- seg::localenv(x = mycoordinates, data = mydf, power = s, useExp = TRUE, scale = FALSE, maxdist = pi,
    sprel = mygeodistances, tol = .Machine$double.eps)

segs <- spatseg(env = myenv, method = "all", useC = TRUE, negative.rm = FALSE, tol = .Machine$double.eps)
print("spatial dissimilarity")
segs@d  #spatial dissimilarity
print("spatial relative diversity")
segs@r  #spatial relative diversity
print("spatial information theory")
segs@h  #spatial information theory
print("Spatial Isolation group 1")
segs@p[1, 1]  #spatial exposure/isolation
print("Spatial Exposure group 1 to 2")
segs@p[1, 2]
print("Spatial Exposure group 2 to 1")
segs@p[2, 1]
print("Spatial Isolation group 2")
segs@p[2, 2]

# spatial proximity
sp <- isp(x = mycoordinates, data = mydf, nb = geodistances, fun = function(x) {
    exp(-x * 1)
})
print("Spatial proximity")
sp
```
SP function test

```{r,eval=FALSE}
SP_test<-fSP(df=mydf,column="V1",distances=geodistances)
print("Spatial proximity")
SP_test

```

```{r,eval=FALSE}
geodistances <- world$matdist
diag(geodistances) <- Inf
SP(x = mydf, d = geodistances, fdist = "e", beta = s)
```

```{r,eval=FALSE}
# run the loop in parallel
n.cores <- parallel::detectCores() - 1
my.cluster <- parallel::makeCluster(n.cores, type = "PSOCK")
doParallel::registerDoParallel(cl = my.cluster)

# something goes wrong with N=100 h[5], pg[5] #yes all groups are same color , thus option
# .errorhandling = remove
dataworldsN1 <- foreach(Nsim = N, i = icount(), .combine = "rbind") %:% foreach(cnsim = cn, j = icount(),
    .combine = "rbind") %:% foreach(hsim = h, k = icount(), .combine = "rbind") %:% foreach(tcsim = tc,
    l = icount(), .combine = "rbind") %:% foreach(pgsim = pg, m = icount(), .combine = "rbind") %:% foreach(ssim = s,
    .packages = packages, n = icount(), .combine = "rbind", .inorder = FALSE, .errorhandling = "remove") %dopar%
    {

        load(paste("./data/processed/worlds/worldN", Nsim, "cn", cnsim, "h", hsim, "tc", tcsim, "pg",
            pgsim, "rda", sep = ""))

        geodistances <- world$matdist
        diag(geodistances) <- 0
        mydf <- as.data.frame(cbind(as.numeric(world$groups == 1), as.numeric(world$groups == -1)))
        mycoordinates <- world$matlonglat
        geodistances <- world$matdist
        diag(geodistances) <- 0
        mygeodistances <- as.dist(geodistances)  #the class of the distance matrix should be dist.              

        myenv <- seg::localenv(x = mycoordinates, data = mydf, power = ssim, useExp = TRUE, scale = FALSE,
            maxdist = pi, sprel = mygeodistances, tol = .Machine$double.eps)

        # PACKAGE SEG
        segs <- spatseg(env = myenv, method = "all", useC = TRUE, negative.rm = FALSE, tol = .Machine$double.eps)
        D <- segs@d
        R <- segs@r
        H <- segs@h
        P_11 <- segs@p[1, 1]
        P_12 <- segs@p[1, 2]
        P_21 <- segs@p[2, 1]
        P_22 <- segs@p[2, 2]

        # Moran's I
        weights <- exp(-geodistances * ssim)
        diag(weights) <- 0  #for Moran we do not want own location. 
        MI <- fMoranI(world$groups, scaled = FALSE, weight = weights, na.rm = TRUE)$observed

        # mean local exposure to outgroup ###not a segregation measure but useful in ABM###
        Eo <- mean(c(myenv@env[, 2][myenv@data[, 1] == 1], myenv@env[, 1][myenv@data[, 2] == 1]))

        # whites spatial proximity index
        SP <- SP(x = mydf, d = geodistances, fdist = "e", beta = ssim)
        
        id <- i * 10000 + j * 1000 + k * 100 + l * 10 + m
        
        #exposure index
        DP<-DPxy(x = mydf, d = geodistances, beta = s)

        
        # SAVE IN DATAFRAME
        data.frame(id = id, s = ssim, N = Nsim, cn = cnsim, h = hsim, tc = tcsim, pg = pgsim, seed = world$seed,
            MI = MI, D = D, R = R, H = H, P_11 = P_11, P_12 = P_12, P_21 = P_21, P_22 = P_22, Eo = Eo,
            SP = SP,DP = DP, i = i, j = j, k = k, l = l, m = m, n = n)

    }
```

```{r,eval=FALSE}
fsave(dataworldsN1, "SegWorlds")
```

#low spatial information theory and high exposure

```{r,eval=FALSE}
load("./data/processed/20220712SegWorlds")
worlds <- x
rm(x)

filter(N==400) %>% 
  filter(P_12<mean(P_12)&H>mean(H)) %>% 
  arrange(H)->worldssel

load(paste("./data/processed/worlds/worldN", N[3], "cn", cn[1], "h", h[2], "tc", tc[1], "pg", pg[3],
    "rda", sep = ""))

world1 <- world
```

```{r,eval=FALSE}
{
    plot3d(world1$coordinatesxyz, col = world1$color, box = FALSE, axes = FALSE, xlab = "", ylab = "",
        zlab = "", size = 4, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
    rgl.spheres(0, 0, 0, radius = 0.99, color = "grey")
}
```

```{r,eval=FALSE}
test <- world1
# first define data.
mydf <- as.data.frame(cbind(as.numeric(test$groups == 1), as.numeric(test$groups == -1)))
# define the coordinates. (note: this are from a sphere)
mycoordinates <- test$matlonglat
mydf$Longitude <- test$matlonglat[, 1]
mydf$Latitude <- test$matlonglat[, 2]
points = st_as_sf(mydf, coords = c("Longitude", "Latitude"), crs = 4326)
graticule = st_graticule(lat = seq(-80, 80, 10), lon = seq(-180, 180, 10))
robinson = "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
projected = st_transform(points, robinson)
graticule = st_transform(graticule, robinson)
{
    plot(projected$geometry, pch = 16, col = test$color, reset = FALSE)
    plot(graticule$geometry, col = "#00000040", add = T)
}
```


#high spatial information theory and high exposure

```{r,eval=FALSE}
load("./data/processed/20220712SegWorlds")
worlds <- x
rm(x)

filter(N==400) %>% 
  filter(P_12>mean(P_12)&H>mean(H)) %>% 
  arrange(H)->worldssel

load(paste("./data/processed/worlds/worldN", N[3], "cn", cn[1], "h", h[3], "tc", tc[1], "pg", pg[2],
    "rda", sep = ""))

world2 <- world
```

```{r,eval=FALSE}
{
    plot3d(world2$coordinatesxyz, col = world2$color, box = FALSE, axes = FALSE, xlab = "", ylab = "",
        zlab = "", size = 4, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
    rgl.spheres(0, 0, 0, radius = 0.99, color = "grey")
}
```

```{r,eval=FALSE}
test <- world2
# first define data.
mydf <- as.data.frame(cbind(as.numeric(test$groups == 1), as.numeric(test$groups == -1)))
# define the coordinates. (note: this are from a sphere)
mycoordinates <- test$matlonglat
mydf$Longitude <- test$matlonglat[, 1]
mydf$Latitude <- test$matlonglat[, 2]
points = st_as_sf(mydf, coords = c("Longitude", "Latitude"), crs = 4326)
graticule = st_graticule(lat = seq(-80, 80, 10), lon = seq(-180, 180, 10))
robinson = "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
projected = st_transform(points, robinson)
graticule = st_transform(graticule, robinson)
{
    plot(projected$geometry, pch = 16, col = test$color, reset = FALSE)
    plot(graticule$geometry, col = "#00000040", add = T)
}
```



#high spatial information theory and low exposure

```{r,eval=FALSE}
load("./data/processed/20220712SegWorlds")
worlds <- x
rm(x)

worlds %>%
  filter(N==400) %>% 
  filter(P_12<mean(P_12)&H>mean(H)) %>% 
  arrange(desc(H))->worldssel

load(paste("./data/processed/worlds/worldN", N[3], "cn", cn[1], "h", h[3], "tc", tc[2], "pg", pg[1],
    "rda", sep = ""))

world3 <- world
```

```{r,eval=FALSE}
{
    plot3d(world3$coordinatesxyz, col = world3$color, box = FALSE, axes = FALSE, xlab = "", ylab = "",
        zlab = "", size = 4, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
    rgl.spheres(0, 0, 0, radius = 0.99, color = "grey")
}
```

```{r,eval=FALSE}
test <- world3
# first define data.
mydf <- as.data.frame(cbind(as.numeric(test$groups == 1), as.numeric(test$groups == -1)))
# define the coordinates. (note: this are from a sphere)
mycoordinates <- test$matlonglat
mydf$Longitude <- test$matlonglat[, 1]
mydf$Latitude <- test$matlonglat[, 2]
points = st_as_sf(mydf, coords = c("Longitude", "Latitude"), crs = 4326)
graticule = st_graticule(lat = seq(-80, 80, 10), lon = seq(-180, 180, 10))
robinson = "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
projected = st_transform(points, robinson)
graticule = st_transform(graticule, robinson)
{
    plot(projected$geometry, pch = 16, col = test$color, reset = FALSE)
    plot(graticule$geometry, col = "#00000040", add = T)
}
```

#low spatial information theory and low exposure
```{r,eval=FALSE}
load("./data/processed/20220712SegWorlds")
worlds <- x
rm(x)

worlds %>%
  filter(N==400) %>% 
  filter(P_12<mean(P_12)&H<mean(H)) %>% 
  arrange(H)->worldssel

load(paste("./data/processed/worlds/worldN", N[3], "cn", cn[1], "h", h[2], "tc", tc[1], "pg", pg[3],
    "rda", sep = ""))

world4 <- world
```

```{r,eval=FALSE}
{
    plot3d(world4$coordinatesxyz, col = world4$color, box = FALSE, axes = FALSE, xlab = "", ylab = "",
        zlab = "", size = 4, xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-1, 1))
    rgl.spheres(0, 0, 0, radius = 0.99, color = "grey")
}
```

```{r,eval=FALSE}
test <- world4
# first define data.
mydf <- as.data.frame(cbind(as.numeric(test$groups == 1), as.numeric(test$groups == -1)))
# define the coordinates. (note: this are from a sphere)
mycoordinates <- test$matlonglat
mydf$Longitude <- test$matlonglat[, 1]
mydf$Latitude <- test$matlonglat[, 2]
points = st_as_sf(mydf, coords = c("Longitude", "Latitude"), crs = 4326)
graticule = st_graticule(lat = seq(-80, 80, 10), lon = seq(-180, 180, 10))
robinson = "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
projected = st_transform(points, robinson)
graticule = st_transform(graticule, robinson)
{
    plot(projected$geometry, pch = 16, col = test$color, reset = FALSE)
    plot(graticule$geometry, col = "#00000040", add = T)
}
```